{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating 4 digit number strings from 0000 to 1119 for the filenames\n",
    "ids = [\"%04d\"%i for i in range(1200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loading the trianing set pickled files X_tr, S_tr, N_tr in stft domain\n",
    "X_tr: clean + noise signals \n",
    "S_tr: clean signals\n",
    "N_tr: noise signals \n",
    "\"\"\"\n",
    "\n",
    "with open('data/signals.pkl', 'rb') as f:\n",
    "    X_tr, S_tr, N_tr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking magnitude and transpose\n",
    "\n",
    "X_tr_mod = [np.abs(signal).T for signal in X_tr]\n",
    "S_tr_mod = [np.abs(signal).T for signal in S_tr]\n",
    "N_tr_mod = [np.abs(signal).T for signal in N_tr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target of the training procedure is Ideal Binary Masks (IBM) and it is constructed in the following way:\n",
    "\n",
    "\\begin{equation*}\n",
    "M_{f,t}^{(l)} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if} & |S^{(l)}_{tr}|_{f,t} > |N^{(l)}_{tr}|_{f,t} \\\\\n",
    "    0 & \\text{if} & |S^{(l)}_{tr}|_{f,t} \\leq |N^{(l)}_{tr}|_{f,t}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "where $l$ denotes a trainng sample.\n",
    "\n",
    "\n",
    "IBM assumes that each of the time-frequency bin at (f,t), an element of $X_{tr}^{(l)}$, is from either speech or noise. Clean speech signal can be recovered using:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{S} = M \\odot X\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing IBM matrix\n",
    "\n",
    "M = []\n",
    "\n",
    "for i in range(1200):\n",
    "    M.append(np.greater(S_tr_mod[i], N_tr_mod[i]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to save/restore model\n",
    "model_path = \"models/rnn-denoise/model.ckpt\"\n",
    "\n",
    "x = tf.placeholder(tf.float32, [10, None, 513])\n",
    "y_ = tf.placeholder(tf.float32, [10, None, 513])\n",
    "\n",
    "hidden_units = 256\n",
    "out_weights = tf.Variable(tf.random_normal([hidden_units, 513], stddev=2/(hidden_units+513), mean=0)) # xavier init\n",
    "out_bias = tf.Variable(tf.zeros([513]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the LSTM cell & n/w\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(hidden_units, initializer=tf.contrib.layers.xavier_initializer())\n",
    "outputs, _ = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add dimension to out_weights so that it can be multiplied with RNN outputs \n",
    "\n",
    "outputs.shape = [10, ?, hidden_units]\n",
    "out_weights.shape = [hidden_units, 513]\n",
    "\n",
    "To multiply them, out_weights needs to be expanded to [10, hidden_units, 513]\n",
    "\"\"\"\n",
    "weights = tf.expand_dims(tf.ones([10, 1]), 1) * out_weights\n",
    "\n",
    "y = tf.nn.sigmoid(tf.matmul(outputs, weights) + out_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse = tf.reduce_mean(tf.losses.mean_squared_error(y_, y))\n",
    "train_step = tf.train.AdamOptimizer().minimize(mse) # adam optimizer with default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() \n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 \tcost=0.226507796\n",
      "Epoch: 02 \tcost=0.198697235\n",
      "Epoch: 03 \tcost=0.183353049\n",
      "Epoch: 04 \tcost=0.169997571\n",
      "Epoch: 05 \tcost=0.162413992\n",
      "Epoch: 06 \tcost=0.155697847\n",
      "Epoch: 07 \tcost=0.151109692\n",
      "Epoch: 08 \tcost=0.147298979\n",
      "Epoch: 09 \tcost=0.144272449\n",
      "Epoch: 10 \tcost=0.141634188\n",
      "Epoch: 11 \tcost=0.139778016\n",
      "Epoch: 12 \tcost=0.137745353\n",
      "Epoch: 13 \tcost=0.135811115\n",
      "Epoch: 14 \tcost=0.134418409\n",
      "Epoch: 15 \tcost=0.133526837\n",
      "Epoch: 16 \tcost=0.131870356\n",
      "Epoch: 17 \tcost=0.131329252\n",
      "Epoch: 18 \tcost=0.129605003\n",
      "Epoch: 19 \tcost=0.128048953\n",
      "Epoch: 20 \tcost=0.126626614\n",
      "Epoch: 21 \tcost=0.125324596\n",
      "Epoch: 22 \tcost=0.124352452\n",
      "Epoch: 23 \tcost=0.123274874\n",
      "Epoch: 24 \tcost=0.122364058\n",
      "Epoch: 25 \tcost=0.121724157\n",
      "Epoch: 26 \tcost=0.120848986\n",
      "Epoch: 27 \tcost=0.120137190\n",
      "Epoch: 28 \tcost=0.119741255\n",
      "Epoch: 29 \tcost=0.118962771\n",
      "Epoch: 30 \tcost=0.118396570\n",
      "Epoch: 31 \tcost=0.118777787\n",
      "Epoch: 32 \tcost=0.117374832\n",
      "Epoch: 33 \tcost=0.116481439\n",
      "Epoch: 34 \tcost=0.116489816\n",
      "Epoch: 35 \tcost=0.115418935\n",
      "Epoch: 36 \tcost=0.114213756\n",
      "Epoch: 37 \tcost=0.113406962\n",
      "Epoch: 38 \tcost=0.112927683\n",
      "Epoch: 39 \tcost=0.112287003\n",
      "Epoch: 40 \tcost=0.111625225\n",
      "Epoch: 41 \tcost=0.111441594\n",
      "Epoch: 42 \tcost=0.110901457\n",
      "Epoch: 43 \tcost=0.110340708\n",
      "Epoch: 44 \tcost=0.109817902\n",
      "Epoch: 45 \tcost=0.109360363\n",
      "Epoch: 46 \tcost=0.111597959\n",
      "Epoch: 47 \tcost=0.113569463\n",
      "Epoch: 48 \tcost=0.110120275\n",
      "Epoch: 49 \tcost=0.108625320\n",
      "Epoch: 50 \tcost=0.107745827\n",
      "Epoch: 51 \tcost=0.106863163\n",
      "Epoch: 52 \tcost=0.107343874\n",
      "Epoch: 53 \tcost=0.106719682\n",
      "Epoch: 54 \tcost=0.105861004\n",
      "Epoch: 55 \tcost=0.105126541\n",
      "Epoch: 56 \tcost=0.104379247\n",
      "Epoch: 57 \tcost=0.104143547\n",
      "Epoch: 58 \tcost=0.103521172\n",
      "Epoch: 59 \tcost=0.103036733\n",
      "Epoch: 60 \tcost=0.102581073\n",
      "Epoch: 61 \tcost=0.103372210\n",
      "Epoch: 62 \tcost=0.103293592\n",
      "Epoch: 63 \tcost=0.103326952\n",
      "Epoch: 64 \tcost=0.102697298\n",
      "Epoch: 65 \tcost=0.101891821\n",
      "Epoch: 66 \tcost=0.101338840\n",
      "Epoch: 67 \tcost=0.100830579\n",
      "Epoch: 68 \tcost=0.100483105\n",
      "Epoch: 69 \tcost=0.100189122\n",
      "Epoch: 70 \tcost=0.099979861\n",
      "Epoch: 71 \tcost=0.099847801\n",
      "Epoch: 72 \tcost=0.099905027\n",
      "Epoch: 73 \tcost=0.099871237\n",
      "Epoch: 74 \tcost=0.100093304\n",
      "Epoch: 75 \tcost=0.100674124\n",
      "Epoch: 76 \tcost=0.100241770\n",
      "Epoch: 77 \tcost=0.099946900\n",
      "Epoch: 78 \tcost=0.099996758\n",
      "Epoch: 79 \tcost=0.100041615\n",
      "Epoch: 80 \tcost=0.099231918\n",
      "Epoch: 81 \tcost=0.098429141\n",
      "Epoch: 82 \tcost=0.097828501\n",
      "Epoch: 83 \tcost=0.097340556\n",
      "Epoch: 84 \tcost=0.096966452\n",
      "Epoch: 85 \tcost=0.096850844\n",
      "Epoch: 86 \tcost=0.097134688\n",
      "Epoch: 87 \tcost=0.097207437\n",
      "Epoch: 88 \tcost=0.098042855\n",
      "Epoch: 89 \tcost=0.097891221\n",
      "Epoch: 90 \tcost=0.097714275\n",
      "Epoch: 91 \tcost=0.097077243\n",
      "Epoch: 92 \tcost=0.096504753\n",
      "Epoch: 93 \tcost=0.096056403\n",
      "Epoch: 94 \tcost=0.095918486\n",
      "Epoch: 95 \tcost=0.096062981\n",
      "Epoch: 96 \tcost=0.096432969\n",
      "Epoch: 97 \tcost=0.096944677\n",
      "Epoch: 98 \tcost=0.096345094\n",
      "Epoch: 99 \tcost=0.095948384\n",
      "Epoch: 100 \tcost=0.096722077\n"
     ]
    }
   ],
   "source": [
    "#train step\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(0, 1200, 10):\n",
    "        batch_x = X_tr_mod[i:i+10] \n",
    "        batch_y = M[i:i+10]\n",
    "        _, cost = sess.run([train_step, mse], feed_dict={x: batch_x, y_: batch_y})\n",
    "        avg_cost += cost/120\n",
    "    print(\"Epoch:\", '%02d'%(epoch+1), \"\\tcost={:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "save_path = saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/rnn-denoise/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# restore trained model to use for validation\n",
    "saver.restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled time domain S_v: clean signals from validation set\n",
    "\n",
    "with open('data/validation_clean.pkl', 'rb') as f:\n",
    "    s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading picked validation set files X_v, S_v, N_v in stft domain\n",
    "\n",
    "with open('data/validation.pkl', 'rb') as f:\n",
    "    X_v, S_v, N_v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking magnitude and transpose\n",
    "\n",
    "X_v_mod = [np.abs(signal).T for signal in X_v]\n",
    "S_v_mod = [np.abs(signal).T for signal in S_v]\n",
    "N_v_mod = [np.abs(signal).T for signal in N_v]\n",
    "X_v_T = [signal.T for signal in X_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_v = []\n",
    "for i in range(1200):\n",
    "    M_v.append(np.greater(S_v_mod[i], N_v_mod[i]).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating SNR (Signal-to-Noise ratio) to check performance of model on validation set. \n",
    "\n",
    "\\begin{equation*}\n",
    "SNR = 10\\log_{10}\\frac{\\sum_t s^2(t)}{\\sum_t (s(t) - \\hat{s}(t))^2}\n",
    "\\end{equation*}\n",
    "\n",
    "If the recovered signal is same as the original clean signal, the denominator will be zero and the SNR becomes infinitely large. Therfore, higher the SNR, the better the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.130348144\n",
      "SNR =  11.051182855168978\n"
     ]
    }
   ],
   "source": [
    "# checking validation loss and calculating snr\n",
    "\n",
    "avg_cost = 0\n",
    "snr = []\n",
    "\n",
    "for i in range(0, 1200, 10):\n",
    "    batch_x = X_v_mod[i:i+10] \n",
    "    batch_y = M_v[i:i+10]\n",
    "\n",
    "    cost, M_hat = sess.run([mse, y], feed_dict={x: batch_x, y_: batch_y})\n",
    "    \n",
    "    avg_cost += cost/120\n",
    "    \n",
    "    batch_x_complex = X_v_T[i:i+10]\n",
    "    batch_s = s[i:i+10]\n",
    "    for j in range(10):\n",
    "        S_hat = np.multiply(M_hat[j], batch_x_complex[j])\n",
    "        s_hat = librosa.istft(S_hat.T, win_length=1024, hop_length=512)\n",
    "        \n",
    "        t = min(len(s_hat), len(batch_s[j]))\n",
    "        snr.append(10*np.log10((np.sum(np.square(batch_s[j][:t])))/np.sum(np.square(batch_s[j][:t]-s_hat[:t]))))\n",
    "\n",
    "print(\"Validation loss = {:.9f}\".format(avg_cost))\n",
    "print(\"SNR = \", sum(snr)/1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filenames of test files (400 test files)\n",
    "te_filenames = ['tex{}.wav'.format(id) for id in ids[:400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled test files X_te in stft domain\n",
    "\n",
    "with open('data/test.pkl', 'rb') as f:\n",
    "    X_te, srs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking magnitude and transpose\n",
    "\n",
    "X_te_mod = [np.abs(signal).T for signal in X_te]\n",
    "X_te_T = [signal.T for signal in X_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reconstructing test signals\n",
    "\n",
    "for i in range(0, 400, 10):\n",
    "    batch_x = X_te_mod[i:i+10] \n",
    "    M_hat = sess.run(y, feed_dict={x: batch_x})\n",
    "    \n",
    "    batch_x_complex = X_te_T[i:i+10]\n",
    "    batch_filenames = te_filenames[i:i+10]\n",
    "    batch_sr = srs[i:i+10]\n",
    "    for j in range(10):\n",
    "        S_hat = np.multiply(M_hat[j], batch_x_complex[j])\n",
    "        s_hat = librosa.istft(S_hat.T, win_length=1024, hop_length=512)\n",
    "        librosa.output.write_wav('outputs/recons_'+batch_filenames[j][-11:], s_hat, srs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
