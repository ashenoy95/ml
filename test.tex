\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{url}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}

\usepackage{tikz}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newcommand{\set}[1]{\left\{#1\right\}}


\lstset{
language=Python,
basicstyle=\tt,
otherkeywords={self},             % Add keywords here
keywordstyle=\tt\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\tt \bf\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}

\newcommand{\E}{\mathbf E}


\parindent=0cm
\parskip=0.3cm

\begin{document}

\textbf{Aniket Shenoy} 

\begin{center}
\large
\textbf{Homework Assignment \#4}
\end{center}

\paragraph{Question 1}
\begin{center}
    \begin{tabular}{| c | c | c | c |}
    \hline
    \textbf{x} & \textbf{y} & \textbf{z} & \textbf{o/p} \\ \hline
    -1 & -1 & -1 & -1 \\ \hline
    -1 & -1 & 1 & -1 \\ \hline
    -1 & 1 & -1 & -1 \\ \hline
    -1 & 1 & 1 & 1 \\ \hline
    1 & -1 & -1 & -1 \\ \hline
    1 & -1 & 1 & -1 \\ \hline
    1 & 1 & -1 & -1 \\ \hline
    1 & 1 & 1 & -1 \\ \hline
    \end{tabular} \\
    \bigskip
    Truth table
\end{center}
For an M-P neuron:
\begin{align*}
    v = & xw \\
    \phi(v) = &
    \begin{cases}
    1, & \text{if $v\geq 0$} \\
    -1, & \text{if $v<0$}
    \end{cases} \\
    y = & \phi(v) \\
    \text{Consider }
    w = & (-1,1,1,-3)^T \\
    \therefore v = &
    \begin{bmatrix}
    -1 & -1 & -1  &1\\ 
    -1 & -1 & 1  &1\\ 
    -1 & 1 & -1  &1\\ 
    -1 & 1 & 1  &1\\ 
    1 & -1 & -1 &1\\ 
    1 & -1 & 1  &1\\ 
    1 & 1 & -1  &1\\ 
    1 & 1 & 1 &1\\ 
    \end{bmatrix}
    \begin{bmatrix}
    -1\\
    1\\
    1\\
    -3
    \end{bmatrix} \\
    = & \begin{bmatrix}
    -4\\
    -2\\
    -2\\
    0\\
    -6\\
    -4\\
    -4\\
    -2
    \end{bmatrix}\\
    \therefore y = &
    \begin{bmatrix}
    -1\\
    -1\\
    -1\\
    1\\
    -1\\
    -1\\
    -1\\
    -1
    \end{bmatrix}
    \intertext{which matches the desired output.}
    \therefore w_x=-1, w_y=1, w_z=1 \text{ and } b=-3
\end{align*}

\paragraph{Question 2}
\begin{align*}
    \intertext{Consider the following definition for an M-P neuron}
    v = & xw \\
    \phi(v) = &
    \begin{cases}
    1, & \text{if $v\geq 0$} \\
    0, & \text{if $v<0$}
    \end{cases} \\
    y = & \phi(v) \\
\end{align*}
\begin{itemize}
    \item [(a)]
    The LSB of the adder, $z$ only depends on $v$ and $x$
    \begin{center}
    \begin{tabular}{| c | c | c |}
    \hline
    \textbf{v} & \textbf{x} & \textbf{z} \\ \hline
    0 & 0 & 0 \\ \hline
    0 & 1 & 1 \\ \hline
    1 & 0 & 1 \\ \hline
    1 & 1 & 0 \\ \hline
    \end{tabular} \\
    \bigskip
    Truth table
    \end{center}
    \begin{align*}
        \therefore z = u \oplus x
    \end{align*}
    X1 XOR X2 can be written as: \\
    X1 XOR X2 = (X1 AND NOT X2) OR (X2 AND NOT X1) 
    
    M-P network:
    \begin{center}
        \includegraphics[scale=.6]{2a.png}
    \end{center}
    The first layer performs the two AND NOT's and the second layer performs the OR.
    
    $w_0=1, w_1=-1, w_2=-1, w_3=1, w_4=1, w_5=1 \\
    b_0=-1, b_1=-1, b_2=-1$
    \item[(b)] 
    The MSB of the adder, $y$ depends on $u$, $v$, $w$ and $x$. 
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    \textbf{u} & \textbf{v} & \textbf{w} & \textbf{x} & \textbf{y} & \textbf{z} \\ \hline
    0 & 0 & 0 & 0 & 0 & 0 \\ \hline
    0 & 0 & 0 & 1 & 0 & 0 \\ \hline
    0 & 0 & 1 & 0 & 0 & 0 \\ \hline
    0 & 0 & 1 & 1 & 0 & 0 \\ \hline
    0 & 1 & 0 & 0 & 0 & 0 \\ \hline
    0 & 1 & 0 & 1 & 0 & 0 \\ \hline
    0 & 1 & 1 & 0 & 0 & 0 \\ \hline
    0 & 1 & 1 & 1 & 0 & 0 \\ \hline
    1 & 0 & 0 & 0 & 0 & 0 \\ \hline
    1 & 0 & 0 & 1 & 0 & 0 \\ \hline
    1 & 0 & 1 & 0 & 0 & 0 \\ \hline
    1 & 0 & 1 & 1 & 0 & 0 \\ \hline
    1 & 1 & 0 & 0 & 0 & 0 \\ \hline
    1 & 1 & 0 & 1 & 0 & 0 \\ \hline
    1 & 1 & 1 & 0 & 0 & 0 \\ \hline
    1 & 1 & 1 & 1 & 0 & 0 \\ \hline
    \end{tabular} \\
    \bigskip
    Truth table
    \end{center}
    $y$ can be written as: 
    \begin{align*}
        y=vx \oplus u \oplus w 
    \end{align*}
    M-P network:
    \begin{center}
        \includegraphics[scale=.4]{2b.png}
    \end{center}
    $w_0=1.w_1=1,w_2=1,w_3=1,w_4=1,w_5=1,w_6=1,w_7=-1,w_8=1,\\ w_9=1,w_{10}=1,w_{11}=1,w_{12}=1,w_{13}=-1 \\
    b_0=-2,b_1=-1,b_2=-2,b_3=-1,b_4=-1,b_5=-2,b_6=-1$
\end{itemize}

\paragraph{Question 3}
\begin{itemize}
    \item[(a)] 
    \begin{center}
        \includegraphics[scale=.75]{3a.png}
    \end{center}
    As it is visible from the plot, the input data patters are linearly separable and according to the Perceptron Convergence Theorem, the net will learn to separate the samples.
    \item[(b)]
    \begin{center}
        \includegraphics[scale=.75]{3b.png}
    \end{center}
    Even after adding the new sample, the data is still linearly separable (as can be seen above). Thus, according to the Perceptron Convergence Theorem, the net will learn to separate the samples. 
\end{itemize}

\paragraph{Question 4}
\begin{itemize}
    \item[(a)]
    \begin{center}
        \includegraphics[scale=.75]{4a.png}
    \end{center}
    \item[(b)] 
    \begin{lstlisting}
        import numpy as np
        
        x = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])
        d = np.array([[-1,-1,1,1]])
        eta = .5
        w = np.zeros((1,3))
        n = 0
        while  n<10:
        	n+=1
        	y = np.dot(w,np.transpose(x))
        	y[y<0] = -1
        	y[y>=0] = 1
        	delta = eta*np.dot((d-y),x)
        	w+=delta
        	print('Iter',n,':','\n','W=',w,'\n')
    \end{lstlisting}
    Iter 1 : 
    $W= [[ 0., -1. ,-2.]]$
    
    Iter 2 : 
    $W= [[ 2. , 0.,  0.]]$
    
    Iter 3 : 
    $W= [[ 2., -1. ,-2.]]$ 
    
    Iter 4 : 
    $W= [[ 3.,  0., -1.]]$ 

    Iter 5 : 
    $W= [[ 3.,  0. ,-1.]]$ 
    
    \item[(c)]
    \begin{align*}
        g(x) = & \sum w_i x_i + b = 0 \\
        g(x)= & 3x_1 - 1 \\
        3x_1 - 1 = & 0 \\
        x_1 = & 1/3
    \end{align*}
    \item[(d)]
    \begin{center}
        \includegraphics[scale=.75]{4d.png}
    \end{center}
    \begin{lstlisting}
        import numpy as np
        
        x = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])
        d = np.array([[-1,1,1,-1]])
        eta = .5
        w = np.zeros((1,3))
        n = 0
        while  n<10:
        	n+=1
        	y = np.dot(w,np.transpose(x))
        	y[y<0] = -1
        	y[y>=0] = 1
        	delta = eta*np.dot((d-y),x)
        	w+=delta
        	print('Iter',n,':','\n','W=',w,'\n')
    \end{lstlisting}
    On applying the perceptron learning rule, the weights keep oscillating between $(0,0,0)^T$ and $(-1,-1,-2)^T$ and the weights never converge. 
    It is also visible from the plot above that the input patterns are not linearly separable. (i.e. decision boundary cannot be determined) 
    Thus, it is evident XOR cannot be implemented using just a single perceptron. At least a two-layer perceptron network would be required. 
\end{itemize}

\paragraph{Question 5}
\begin{center}
        \includegraphics[scale=.175]{5.jpg}
\end{center}
The classifier consists of a two-layer feedforward network having a hidden layer with 12 neurons and an output layer with three output units. The input consists of $x_1, x_2$ and a bias $+1$.

Input: \\
The inputs and bias are fully-connected to the hidden layer units. 

Hidden layer: \\
Each neuron represents the AND function. \\
The 1st 4 neurons in the hidden layer check for the decision boundary for class 1.\\
$x1>1, x1<8, x2>0, x2<1$ \\
If $y_1=1$ for the next 3 neurons, then the input belongs to Class 1. \\
Similarly, there are 3 decision boundaries for Class 2: \\
$x_2>3,x_2<5, 2x_1 + 3x_2>15$ \\
If $y_2=1$ then the input belongs to Class 2. \\
Finally, for Class 3, the decision boundaries are: \\
$x_1<0,x_1>-4,x_2>0,x_2<2$ \\
If $y_3=1$ for the next 4 neurons, then the input belongs to Class 3. \\
If $y_i=-1$ for Class $i(1 \leq i \geq 3)$, then the input belongs to Class 4. [as given]

\paragraph{References}
\begin{itemize}
    \item \url{http://www.cs.uccs.edu/~jkalita/work/cs587/2014/02ThresholdLogic.pdf}
    \item \url{http://www.electronics-tutorials.ws/combination/comb_7.html}
\end{itemize}
Worked with Shashi and Siddarth.

\end{document}

